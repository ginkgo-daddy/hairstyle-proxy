import http.client
import json
import os
import mimetypes
from codecs import encode
import time
import requests
from datetime import datetime
from docx import Document
from docx.shared import Inches
from PIL import Image, ExifTags
import io
import random
import concurrent.futures
import threading
from queue import Queue
import base64
import asyncio
import hashlib
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()


def ensure_data_directory():
    """ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨å¹¶æœ‰é€‚å½“çš„æƒé™"""
    data_dir = os.environ.get('RAILWAY_VOLUME_MOUNT_PATH', '/data')
    try:
        if not os.path.exists(data_dir):
            os.makedirs(data_dir, exist_ok=True)
            print(f"åˆ›å»ºæ•°æ®ç›®å½•: {data_dir}")
        
        # æ£€æŸ¥ç›®å½•æƒé™
        if not os.access(data_dir, os.W_OK):
            print(f"è­¦å‘Š: æ•°æ®ç›®å½• {data_dir} æ²¡æœ‰å†™æƒé™")
        else:
            print(f"æ•°æ®ç›®å½•å°±ç»ª: {data_dir}")
            
        return data_dir
    except Exception as e:
        print(f"åˆå§‹åŒ–æ•°æ®ç›®å½•å¤±è´¥: {e}")
        # å›é€€åˆ°å½“å‰ç›®å½•
        fallback_dir = os.path.join(os.getcwd(), 'data')
        os.makedirs(fallback_dir, exist_ok=True)
        print(f"ä½¿ç”¨å›é€€æ•°æ®ç›®å½•: {fallback_dir}")
        return fallback_dir

class HairstyleProcessor:
    def __init__(self, api_key=None, webapp_id=None, color_webapp_id=None, max_workers=30, task_timeout=600):
        # é¦–å…ˆç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        self.data_dir = ensure_data_directory()

        # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
        self.api_key = api_key or os.environ.get('RUNNINGHUB_API_KEY')
        if not self.api_key:
            raise ValueError("API key is required. Set RUNNINGHUB_API_KEY environment variable or pass api_key parameter.")

        # ä»ç¯å¢ƒå˜é‡è·å–Webapp IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
        self.webapp_id = webapp_id or os.environ.get('RUNNINGHUB_WEBAPP_ID')

        # ä»ç¯å¢ƒå˜é‡è·å–é¢œè‰²æ¢è£…Webapp ID
        self.color_webapp_id = color_webapp_id or os.environ.get('RUNNINGHUB_COLOR_WEBAPP_ID')

        # ä»ç¯å¢ƒå˜é‡è·å–OpenRouter APIå¯†é’¥ï¼ˆç”¨äºGeminié¢„å¤„ç†ï¼‰
        self.openrouter_api_key = os.environ.get('OPENROUTER_API_KEY')

        self.host = "www.runninghub.cn"
        self.results = []
        self.results_lock = threading.Lock()
        self.max_workers = max_workers
        self.task_timeout = task_timeout  # æ¯ä¸ªä»»åŠ¡çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤600ç§’

        # æ·»åŠ æ—¶é—´ç»Ÿè®¡å˜é‡
        self.task_times = []  # å­˜å‚¨æ¯æ¬¡run_hairstyle_taskçš„è¿è¡Œæ—¶é—´
        self.task_count = 0   # ä»»åŠ¡æ€»æ•°ç»Ÿè®¡

        # Geminié¢„å¤„ç†ç»Ÿè®¡
        self.gemini_times = []  # å­˜å‚¨Geminié¢„å¤„ç†æ—¶é—´
        self.gemini_success_count = 0  # æˆåŠŸé¢„å¤„ç†æ•°é‡
        self.gemini_fail_count = 0     # å¤±è´¥é¢„å¤„ç†æ•°é‡

        # è¶…æ—¶ç»Ÿè®¡
        self.timeout_count = 0  # è¶…æ—¶ä»»åŠ¡æ•°é‡

    def encode_image(self, image_path):
        """å°†å›¾åƒç¼–ç ä¸ºbase64å­—ç¬¦ä¸²ï¼Œè‡ªåŠ¨å¤„ç†EXIFæ–¹å‘"""
        try:
            # ä½¿ç”¨PILæ‰“å¼€å›¾åƒå¹¶è‡ªåŠ¨å¤„ç†EXIFæ–¹å‘
            with Image.open(image_path) as img:
                # è‡ªåŠ¨æ ¹æ®EXIFæ–¹å‘ä¿¡æ¯æ—‹è½¬å›¾åƒ
                img = self.fix_image_orientation(img)

                # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆé¿å…PNGä¿å­˜é—®é¢˜ï¼‰
                if img.mode != 'RGB':
                    img = img.convert('RGB')

                # ä¿å­˜åˆ°å†…å­˜ç¼“å†²åŒº
                buffer = io.BytesIO()
                img.save(buffer, format='JPEG', quality=95)
                buffer.seek(0)

                # ç¼–ç ä¸ºbase64
                return base64.b64encode(buffer.getvalue()).decode('utf-8')

        except Exception as e:
            print(f"å¤„ç†å›¾åƒEXIFæ–¹å‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•: {e}")
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')

    def fix_image_orientation(self, img):
        """æ ¹æ®EXIFä¿¡æ¯ä¿®æ­£å›¾åƒæ–¹å‘"""
        try:
            # ä½¿ç”¨PILçš„ImageOps.exif_transposeæ–¹æ³•ï¼Œè¿™æ˜¯å¤„ç†EXIFæ–¹å‘çš„æ¨èæ–¹æ³•
            from PIL import ImageOps
            img = ImageOps.exif_transpose(img)
            return img
        except ImportError:
            # å¦‚æœImageOpsä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
            try:
                exif = img._getexif()
                if exif is not None:
                    # æŸ¥æ‰¾æ–¹å‘æ ‡ç­¾
                    for tag, value in exif.items():
                        if ExifTags.TAGS.get(tag) == 'Orientation':
                            # æ ¹æ®EXIFæ–¹å‘å€¼æ—‹è½¬å›¾åƒ
                            if value == 2:
                                img = img.transpose(Image.FLIP_LEFT_RIGHT)
                            elif value == 3:
                                img = img.rotate(180, expand=True)
                            elif value == 4:
                                img = img.transpose(Image.FLIP_TOP_BOTTOM)
                            elif value == 5:
                                img = img.transpose(Image.FLIP_LEFT_RIGHT).rotate(90, expand=True)
                            elif value == 6:
                                img = img.rotate(-90, expand=True)
                            elif value == 7:
                                img = img.transpose(Image.FLIP_LEFT_RIGHT).rotate(-90, expand=True)
                            elif value == 8:
                                img = img.rotate(90, expand=True)
                            break
            except Exception as e:
                print(f"ä¿®æ­£å›¾åƒæ–¹å‘å¤±è´¥: {e}")

        return img

    def get_file_hash(self, file_path):
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            print(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥: {e}")
            return None

    def save_image_from_base64(self, base64_str, original_path, image_type, file_hash):
        """ä»base64å­—ç¬¦ä¸²è¿˜åŸå›¾ç‰‡å¹¶ä¿å­˜ï¼Œä½¿ç”¨å“ˆå¸Œå€¼å‘½å"""
        try:
            output_dir = os.path.join(self.data_dir, f"gemini_processed_{image_type}")
            if not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)

            # ä½¿ç”¨æ–‡ä»¶å“ˆå¸Œå€¼ä½œä¸ºä¸»è¦æ ‡è¯†ç¬¦ï¼Œä¿ç•™åŸæ–‡ä»¶åç”¨äºè¯†åˆ«
            original_filename = os.path.basename(original_path)
            name_without_ext = os.path.splitext(original_filename)[0]

            # æ–‡ä»¶åæ ¼å¼: åŸå_å“ˆå¸Œå‰8ä½_gemini_processed.png
            new_filename = f"{name_without_ext}_{file_hash[:8]}_gemini_processed.png"
            filepath = os.path.join(output_dir, new_filename)

            image_data = base64.b64decode(base64_str)
            with open(filepath, "wb") as f:
                f.write(image_data)

            # åˆ›å»ºç¼“å­˜ç´¢å¼•æ–‡ä»¶
            self.update_cache_index(original_path, filepath, file_hash, image_type)

            return filepath
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {e}")
            return None

    def update_cache_index(self, original_path, processed_path, file_hash, image_type):
        """æ›´æ–°ç¼“å­˜ç´¢å¼•æ–‡ä»¶"""
        try:
            cache_dir = os.path.join(self.data_dir, f"gemini_processed_{image_type}")
            cache_index_path = os.path.join(cache_dir, "cache_index.json")

            # è¯»å–ç°æœ‰ç´¢å¼•
            cache_index = {}
            if os.path.exists(cache_index_path):
                try:
                    with open(cache_index_path, 'r', encoding='utf-8') as f:
                        cache_index = json.load(f)
                except:
                    cache_index = {}

            # æ›´æ–°ç´¢å¼•
            cache_index[file_hash] = {
                "original_path": original_path,
                "processed_path": processed_path,
                "timestamp": datetime.now().isoformat(),
                "original_filename": os.path.basename(original_path)
            }

            # ä¿å­˜ç´¢å¼•
            with open(cache_index_path, 'w', encoding='utf-8') as f:
                json.dump(cache_index, f, ensure_ascii=False, indent=2)

        except Exception as e:
            print(f"æ›´æ–°ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")

    def get_cached_processed_path(self, original_path, image_type):
        """åŸºäºæ–‡ä»¶å“ˆå¸Œæ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„é¢„å¤„ç†å›¾ç‰‡"""
        try:
            # è®¡ç®—åŸæ–‡ä»¶å“ˆå¸Œ
            file_hash = self.get_file_hash(original_path)
            if not file_hash:
                return None

            # æ£€æŸ¥ç¼“å­˜ç´¢å¼•
            cache_dir = os.path.join(self.data_dir, f"gemini_processed_{image_type}")
            cache_index_path = os.path.join(cache_dir, "cache_index.json")

            if not os.path.exists(cache_index_path):
                return None

            # è¯»å–ç¼“å­˜ç´¢å¼•
            try:
                with open(cache_index_path, 'r', encoding='utf-8') as f:
                    cache_index = json.load(f)
            except:
                return None

            # æŸ¥æ‰¾åŒ¹é…çš„å“ˆå¸Œ
            if file_hash in cache_index:
                cached_info = cache_index[file_hash]
                cached_path = cached_info["processed_path"]

                # éªŒè¯ç¼“å­˜æ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨
                if os.path.exists(cached_path):
                    return cached_path
                else:
                    # ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ¸…ç†ç´¢å¼•
                    del cache_index[file_hash]
                    with open(cache_index_path, 'w', encoding='utf-8') as f:
                        json.dump(cache_index, f, ensure_ascii=False, indent=2)

            return None

        except Exception as e:
            print(f"æ£€æŸ¥ç¼“å­˜å¤±è´¥: {e}")
            return None

    async def preprocess_image_with_gemini(self, image_path, image_type="user"):
        """ä½¿ç”¨Geminiå¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        thread_name = threading.current_thread().name
        start_time = time.time()

        try:
            print(f"[{thread_name}] å¼€å§‹Geminié¢„å¤„ç†{image_type}å›¾åƒ: {os.path.basename(image_path)}")

            # æ£€æŸ¥ç¼“å­˜ï¼ˆåŸºäºæ–‡ä»¶å“ˆå¸Œï¼‰
            cached_path = self.get_cached_processed_path(image_path, image_type)
            if cached_path:
                print(f"[{thread_name}] âœ“ æ‰¾åˆ°ç¼“å­˜çš„{image_type}å›¾åƒ: {os.path.basename(cached_path)}")
                return cached_path

            if not self.openrouter_api_key:
                print(f"[{thread_name}] æœªè®¾ç½®OPENROUTER_API_KEYï¼Œè·³è¿‡Geminié¢„å¤„ç†")
                self.gemini_fail_count += 1
                return image_path

            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼ˆç”¨äºä¿å­˜æ—¶çš„æ–‡ä»¶å‘½åï¼‰
            file_hash = self.get_file_hash(image_path)
            if not file_hash:
                print(f"[{thread_name}] æ— æ³•è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼Œè·³è¿‡é¢„å¤„ç†")
                self.gemini_fail_count += 1
                return image_path

            base64_image = self.encode_image(image_path)

            async with AsyncOpenAI(
                base_url="https://openrouter.ai/api/v1",
                api_key=self.openrouter_api_key,
            ) as client:

                # æ ¹æ®å›¾ç‰‡ç±»å‹è®¾ç½®ä¸åŒçš„æç¤ºè¯­
                if image_type == "user":
                    prompt_text = "ä¿æŒäººç‰©ä¸€è‡´æ€§ï¼Œä¿æŒæœé¥°å’Œå‘å‹ä¸å˜ï¼Œèº«æä¸è¦å¤ªèƒ–ï¼Œæ”¹ä¸ºåŠèº«è¯ä»¶ç…§ï¼Œå…‰çº¿å……è¶³ï¼Œéœ²å‡ºé»‘è‰²è…°å¸¦ã€‚"
                elif image_type == "hairstyle":
                    prompt_text = "ä¿æŒäººç‰©ä¸€è‡´æ€§ï¼Œä¿æŒæœé¥°å’Œå‘å‹å‘è‰²ä¸å˜ï¼Œä¿æŒå‘å‹çº¹ç†æ¸…æ™°ï¼Œå…‰ç…§æ¡ä»¶ä¸åŸå›¾ä¸€è‡´ï¼Œæ”¹ä¸ºåŠèº«è¯ä»¶ç…§ï¼Œéœ²å‡ºé»‘è‰²è…°å¸¦ã€‚"
                else:
                    prompt_text = "ä¿æŒäººç‰©ä¸€è‡´æ€§ï¼Œä¿æŒæœé¥°å’Œå‘å‹å‘è‰²ä¸å˜ï¼Œä¿æŒå‘å‹çº¹ç†æ¸…æ™°ï¼Œå…‰ç…§æ¡ä»¶ä¸åŸå›¾ä¸€è‡´ï¼Œæ”¹ä¸ºåŠèº«è¯ä»¶ç…§ï¼Œéœ²å‡ºé»‘è‰²è…°å¸¦ã€‚"

                completion = await client.chat.completions.create(
                    model="google/gemini-2.5-flash-image-preview",
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": prompt_text
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/jpeg;base64,{base64_image}"
                                    }
                                }
                            ]
                        }
                    ]
                )

                end_time = time.time()
                elapsed = end_time - start_time
                self.gemini_times.append(elapsed)
                print(f"[{thread_name}] Geminié¢„å¤„ç†{image_type}è€—æ—¶: {elapsed:.2f}ç§’")

                return await self.process_gemini_response(completion, image_path, image_type, file_hash, thread_name, client, prompt_text, base64_image, attempt=1)

        except Exception as e:
            end_time = time.time()
            elapsed = end_time - start_time
            self.gemini_times.append(elapsed)
            print(f"[{thread_name}] Geminié¢„å¤„ç†å‡ºé”™: {e}")
            print(f"[{thread_name}] ä½¿ç”¨åŸå›¾ç»§ç»­å¤„ç†...")
            self.gemini_fail_count += 1
            return image_path

    async def process_gemini_response(self, completion, image_path, image_type, file_hash, thread_name, client, prompt_text, base64_image, attempt=1):
        """å¤„ç†Gemini APIå“åº”ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
        max_retries = 2  # æœ€å¤šé‡è¯•1æ¬¡ï¼Œæ€»å…±2æ¬¡å°è¯•

        # æ£€æŸ¥å“åº”ä¸­æ˜¯å¦æœ‰å›¾ç‰‡æ•°æ®
        if hasattr(completion.choices[0].message, 'images') and completion.choices[0].message.images:
            image_url = completion.choices[0].message.images[0]["image_url"]['url']

            if image_url.startswith("data:image/"):
                base64_data = image_url.split(",")[1]
                processed_image_path = self.save_image_from_base64(
                    base64_data,
                    image_path,    # åŸå§‹è·¯å¾„
                    image_type,    # å›¾åƒç±»å‹
                    file_hash      # æ–‡ä»¶å“ˆå¸Œ
                )

                if processed_image_path:
                    print(f"[{thread_name}] âœ“ Gemini{image_type}é¢„å¤„ç†æˆåŠŸ: {os.path.basename(processed_image_path)}")
                    self.gemini_success_count += 1
                    return processed_image_path
                else:
                    print(f"[{thread_name}] ä¿å­˜å¤±è´¥ï¼Œä½¿ç”¨åŸå›¾")
                    self.gemini_fail_count += 1
                    return image_path
            else:
                print(f"[{thread_name}] ébase64æ ¼å¼URLï¼Œä½¿ç”¨åŸå›¾")
                self.gemini_fail_count += 1
                return image_path
        else:
            # å“åº”ä¸­æ— å›¾ç‰‡æ•°æ®ï¼Œå°è¯•é‡è¯•
            if attempt < max_retries:
                print(f"[{thread_name}] å“åº”ä¸­æ— å›¾ç‰‡æ•°æ®ï¼Œè¿›è¡Œç¬¬{attempt + 1}æ¬¡å°è¯•...")
                try:
                    # ç­‰å¾…ä¸€å°æ®µæ—¶é—´å†é‡è¯•
                    await asyncio.sleep(1)

                    # é‡æ–°è°ƒç”¨API
                    retry_completion = await client.chat.completions.create(
                        model="google/gemini-2.5-flash-image-preview",
                        messages=[
                            {
                                "role": "user",
                                "content": [
                                    {
                                        "type": "text",
                                        "text": prompt_text
                                    },
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/jpeg;base64,{base64_image}"
                                        }
                                    }
                                ]
                            }
                        ]
                    )

                    print(f"[{thread_name}] é‡è¯•è¯·æ±‚å®Œæˆï¼Œå¤„ç†å“åº”...")
                    # é€’å½’è°ƒç”¨å¤„ç†é‡è¯•çš„å“åº”
                    return await self.process_gemini_response(
                        retry_completion, image_path, image_type, file_hash,
                        thread_name, client, prompt_text, base64_image, attempt + 1
                    )

                except Exception as retry_error:
                    print(f"[{thread_name}] é‡è¯•è¯·æ±‚å¤±è´¥: {retry_error}")
                    print(f"[{thread_name}] ä½¿ç”¨åŸå›¾")
                    self.gemini_fail_count += 1
                    return image_path
            else:
                print(f"[{thread_name}] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå“åº”ä¸­ä»æ— å›¾ç‰‡æ•°æ®ï¼Œä½¿ç”¨åŸå›¾")
                self.gemini_fail_count += 1
                return image_path

    def preprocess_images_concurrently(self, user_image_path, hairstyle_image_path):
        """å¹¶å‘é¢„å¤„ç†ç”¨æˆ·å›¾ç‰‡å’Œå‘å‹å›¾ç‰‡ï¼ˆåŒæ­¥æ¥å£ï¼‰"""
        thread_name = threading.current_thread().name
        try:
            print(f"[{thread_name}] å¼€å§‹å¹¶å‘é¢„å¤„ç†å›¾åƒ...")

            # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯æ¥è¿è¡Œå¼‚æ­¥ä»£ç 
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            try:
                # å¹¶å‘æ‰§è¡Œä¸¤ä¸ªé¢„å¤„ç†ä»»åŠ¡
                processed_user_image, processed_hairstyle_image = loop.run_until_complete(
                    asyncio.gather(
                        self.preprocess_image_with_gemini(user_image_path, "user"),
                        self.preprocess_image_with_gemini(hairstyle_image_path, "hairstyle"),
                        return_exceptions=True
                    )
                )

                # å¤„ç†å¯èƒ½çš„å¼‚å¸¸ç»“æœ
                if isinstance(processed_user_image, Exception):
                    print(f"[{thread_name}] ç”¨æˆ·å›¾åƒé¢„å¤„ç†å¤±è´¥: {processed_user_image}")
                    processed_user_image = user_image_path

                if isinstance(processed_hairstyle_image, Exception):
                    print(f"[{thread_name}] å‘å‹å›¾åƒé¢„å¤„ç†å¤±è´¥: {processed_hairstyle_image}")
                    processed_hairstyle_image = hairstyle_image_path

                print(f"[{thread_name}] å›¾åƒé¢„å¤„ç†å®Œæˆ")
                return processed_user_image, processed_hairstyle_image

            finally:
                loop.close()

        except Exception as e:
            print(f"[{thread_name}] å¹¶å‘é¢„å¤„ç†å¤±è´¥: {e}")
            print(f"[{thread_name}] ä½¿ç”¨åŸå›¾ç»§ç»­...")
            return user_image_path, hairstyle_image_path

    def upload_image(self, image_path):
        """Upload image to RunningHub server and return fileName"""
        corrected_path = image_path
        
        conn = http.client.HTTPSConnection(self.host)
        dataList = []
        boundary = 'wL36Yn8afVp8Ag7AmP8qZ0SA4n1v9T'
        
        dataList.append(encode('--' + boundary))
        dataList.append(encode('Content-Disposition: form-data; name=apiKey;'))
        dataList.append(encode('Content-Type: {}'.format('text/plain')))
        dataList.append(encode(''))
        dataList.append(encode(self.api_key))
        
        dataList.append(encode('--' + boundary))
        filename = os.path.basename(corrected_path)
        dataList.append(encode('Content-Disposition: form-data; name=file; filename={0}'.format(filename)))
        
        fileType = mimetypes.guess_type(corrected_path)[0] or 'application/octet-stream'
        dataList.append(encode('Content-Type: {}'.format(fileType)))
        dataList.append(encode(''))
        
        with open(corrected_path, 'rb') as f:
            dataList.append(f.read())
            
        dataList.append(encode('--' + boundary))
        dataList.append(encode('Content-Disposition: form-data; name=fileType;'))
        dataList.append(encode('Content-Type: {}'.format('text/plain')))
        dataList.append(encode(''))
        dataList.append(encode("image"))
        dataList.append(encode('--'+boundary+'--'))
        dataList.append(encode(''))
        
        body = b'\r\n'.join(dataList)
        headers = {
            'Host': self.host,
            'Content-type': 'multipart/form-data; boundary={}'.format(boundary)
        }
        
        try:
            conn.request("POST", "/task/openapi/upload", body, headers)
            res = conn.getresponse()
            data = res.read()
            result = json.loads(data.decode("utf-8"))
            
            if result.get("code") == 0:
                print(f"Upload successful for {image_path}: {result['data']['fileName']}")
                return result["data"]["fileName"]
            else:
                print(f"Upload failed for {image_path}: {result}")
                print(f"API Response: {result}")
                return None
        except Exception as e:
            print(f"Error uploading {image_path}: {e}")
            return None
        finally:
            conn.close()
            # Clean up temporary corrected file if it was created
            if corrected_path != image_path and os.path.exists(corrected_path):
                try:
                    os.remove(corrected_path)
                except:
                    pass
    
    def run_hairstyle_task(self, hairstyle_filename, user_filename, max_retries=10, retry_delay=20, cancel_check_func=None):
        """Run AI hairstyle transfer task with retry mechanism for TASK_QUEUE_MAXED"""
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        
        payload = json.dumps({
            "webappId": self.webapp_id,
            "apiKey": self.api_key,
            "nodeInfoList": [
                {
                    "nodeId": "901",
                    "fieldName": "image",
                    "fieldValue": hairstyle_filename,
                    "description": "hair"
                },
                {
                    "nodeId": "239",
                    "fieldName": "image",
                    "fieldValue": user_filename,
                    "description": "user"
                }
            ],
        })

        headers = {
            'Host': self.host,
            'Content-Type': 'application/json'
        }

        for attempt in range(max_retries):
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
            if cancel_check_func and cancel_check_func():
                print(f"ä»»åŠ¡åœ¨æ’é˜Ÿé˜¶æ®µè¢«å–æ¶ˆ (attempt {attempt + 1}/{max_retries})")
                return None

            conn = http.client.HTTPSConnection(self.host)
            try:
                conn.request("POST", "/task/openapi/ai-app/run", payload, headers)
                res = conn.getresponse()
                data = res.read()
                result = json.loads(data.decode("utf-8"))

                if result.get("code") == 0:
                    end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
                    elapsed_time = end_time - start_time
                    self.task_times.append(elapsed_time)
                    self.task_count += 1
                    print(f"Task started successfully: {result['data']['taskId']} (è€—æ—¶: {elapsed_time:.2f}ç§’)")
                    return result["data"]["taskId"]
                elif result.get("msg") in ["TASK_QUEUE_MAXED", "TASK_INSTANCE_MAXED"]:
                    print(f"Task queue is full (attempt {attempt + 1}/{max_retries}), waiting {retry_delay} seconds before retry...")
                    if attempt < max_retries - 1:  # Don't sleep on the last attempt
                        # åœ¨ç¡çœ æœŸé—´ä¹Ÿè¦æ£€æŸ¥å–æ¶ˆçŠ¶æ€
                        for i in range(retry_delay):
                            if cancel_check_func and cancel_check_func():
                                print(f"ä»»åŠ¡åœ¨ç­‰å¾…é‡è¯•æœŸé—´è¢«å–æ¶ˆ")
                                return None
                            time.sleep(1)
                        continue
                    else: 
                        end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´ï¼ˆå¤±è´¥æ—¶ï¼‰
                        elapsed_time = end_time - start_time
                        self.task_times.append(elapsed_time)
                        self.task_count += 1
                        print(f"Max retries reached, task queue still full (æ€»è€—æ—¶: {elapsed_time:.2f}ç§’)")
                        return None
                else:
                    end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´ï¼ˆå¤±è´¥æ—¶ï¼‰
                    elapsed_time = end_time - start_time
                    self.task_times.append(elapsed_time)
                    self.task_count += 1
                    print(f"Task failed: {result} (è€—æ—¶: {elapsed_time:.2f}ç§’)")
                    print(f"API Response: {result}")
                    return None
            except Exception as e:
                end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´ï¼ˆå¼‚å¸¸æ—¶ï¼‰
                elapsed_time = end_time - start_time
                self.task_times.append(elapsed_time)
                self.task_count += 1
                print(f"Error running task (attempt {attempt + 1}/{max_retries}): {e} (è€—æ—¶: {elapsed_time:.2f}ç§’)")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                else:
                    return None
            finally:
                conn.close()

        return None

    def run_color_task(self, hair_filename, user_filename, max_retries=10, retry_delay=20, cancel_check_func=None):
        """Run AI color transfer task with retry mechanism for TASK_QUEUE_MAXED"""
        if not self.color_webapp_id:
            raise ValueError("Color webapp ID is required. Set RUNNINGHUB_COLOR_WEBAPP_ID environment variable.")

        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´

        payload = json.dumps({
            "webappId": self.color_webapp_id,
            "apiKey": self.api_key,
            "nodeInfoList": [
                {
                    "nodeId": "1",
                    "fieldName": "image",
                    "fieldValue": user_filename,
                    "description": "user"
                },
                {
                    "nodeId": "200",
                    "fieldName": "image",
                    "fieldValue": hair_filename,
                    "description": "hair"
                }
            ],
        })

        headers = {
            'Host': self.host,
            'Content-Type': 'application/json'
        }

        for attempt in range(max_retries):
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
            if cancel_check_func and cancel_check_func():
                print(f"é¢œè‰²æ¢è£…ä»»åŠ¡åœ¨æ’é˜Ÿé˜¶æ®µè¢«å–æ¶ˆ (attempt {attempt + 1}/{max_retries})")
                return None

            conn = http.client.HTTPSConnection(self.host)
            try:
                conn.request("POST", "/task/openapi/ai-app/run", payload, headers)
                res = conn.getresponse()
                data = res.read()
                result = json.loads(data.decode("utf-8"))

                if result.get("code") == 0:
                    end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
                    elapsed_time = end_time - start_time
                    self.task_times.append(elapsed_time)
                    self.task_count += 1
                    print(f"Color task started successfully: {result['data']['taskId']} (è€—æ—¶: {elapsed_time:.2f}ç§’)")
                    return result["data"]["taskId"]
                elif result.get("msg") in ["TASK_QUEUE_MAXED", "TASK_INSTANCE_MAXED"]:
                    print(f"Color task queue is full (attempt {attempt + 1}/{max_retries}), waiting {retry_delay} seconds before retry...")
                    if attempt < max_retries - 1:  # Don't sleep on the last attempt
                        # åœ¨ç¡çœ æœŸé—´ä¹Ÿè¦æ£€æŸ¥å–æ¶ˆçŠ¶æ€
                        for i in range(retry_delay):
                            if cancel_check_func and cancel_check_func():
                                print(f"é¢œè‰²æ¢è£…ä»»åŠ¡åœ¨ç­‰å¾…é‡è¯•æœŸé—´è¢«å–æ¶ˆ")
                                return None
                            time.sleep(1)
                        continue
                    else:
                        end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´ï¼ˆå¤±è´¥æ—¶ï¼‰
                        elapsed_time = end_time - start_time
                        self.task_times.append(elapsed_time)
                        self.task_count += 1
                        print(f"Max retries reached, color task queue still full (æ€»è€—æ—¶: {elapsed_time:.2f}ç§’)")
                        return None
                else:
                    end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´ï¼ˆå¤±è´¥æ—¶ï¼‰
                    elapsed_time = end_time - start_time
                    self.task_times.append(elapsed_time)
                    self.task_count += 1
                    print(f"Color task failed: {result} (è€—æ—¶: {elapsed_time:.2f}ç§’)")
                    print(f"API Response: {result}")
                    return None
            except Exception as e:
                end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´ï¼ˆå¼‚å¸¸æ—¶ï¼‰
                elapsed_time = end_time - start_time
                self.task_times.append(elapsed_time)
                self.task_count += 1
                print(f"Error running color task (attempt {attempt + 1}/{max_retries}): {e} (è€—æ—¶: {elapsed_time:.2f}ç§’)")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                else:
                    return None
            finally:
                conn.close()

        return None
    
    def check_task_status(self, task_id):
        """Check task status"""
        conn = http.client.HTTPSConnection(self.host)
        payload = json.dumps({
            "apiKey": self.api_key,
            "taskId": task_id
        })
        
        headers = {
            'Host': self.host,
            'Content-Type': 'application/json'
        }
        
        try:
            conn.request("POST", "/task/openapi/status", payload, headers)
            res = conn.getresponse()
            data = res.read()
            result = json.loads(data.decode("utf-8"))
            
            if result.get("code") == 0:
                return result["data"]
            else:
                print(f"Status check failed: {result}")
                return None
        except Exception as e:
            print(f"Error checking status: {e}")
            return None
        finally:
            conn.close()
    
    def get_task_results(self, task_id):
        """Get task results"""
        conn = http.client.HTTPSConnection(self.host)
        payload = json.dumps({
            "apiKey": self.api_key,
            "taskId": task_id
        })

        headers = {
            'Host': self.host,
            'Content-Type': 'application/json'
        }

        try:
            conn.request("POST", "/task/openapi/outputs", payload, headers)
            res = conn.getresponse()
            data = res.read()
            result = json.loads(data.decode("utf-8"))

            if result.get("code") == 0:
                return result["data"]
            else:
                print(f"Get results failed: {result}")
                return None
        except Exception as e:
            print(f"Error getting results: {e}")
            return None
        finally:
            conn.close()

    def cancel_task(self, task_id):
        """Cancel task"""
        conn = http.client.HTTPSConnection(self.host)
        payload = json.dumps({
            "apiKey": self.api_key,
            "taskId": task_id
        })

        headers = {
            'Host': self.host,
            'Content-Type': 'application/json'
        }

        try:
            conn.request("POST", "/task/openapi/cancel", payload, headers)
            res = conn.getresponse()
            data = res.read()
            result = json.loads(data.decode("utf-8"))

            if result.get("code") == 0:
                print(f"Task cancelled successfully: {task_id}")
                return True
            else:
                print(f"Cancel task failed: {result}")
                return False
        except Exception as e:
            print(f"Error cancelling task: {e}")
            return False
        finally:
            conn.close()
    
    def download_image(self, url, save_path):
        """Download image from URL"""
        try:
            response = requests.get(url)
            if response.status_code == 200:
                with open(save_path, 'wb') as f:
                    f.write(response.content)
                return True
            else:
                print(f"Failed to download {url}")
                return False
        except Exception as e:
            print(f"Error downloading {url}: {e}")
            return False
    
    def create_combined_image(self, hairstyle_path, user_path, result_paths, output_path):
        """Create a combined image with hairstyle reference, user photo, and all generated results"""
        try:
            # Open hairstyle and user images
            hairstyle_img = Image.open(hairstyle_path)
            user_img = Image.open(user_path)
            
            # Open all result images
            result_imgs = []
            for result_path in result_paths:
                if os.path.exists(result_path):
                    result_imgs.append(Image.open(result_path))
            
            if not result_imgs:
                print("No result images found")
                return False
            
            # Collect all images
            all_imgs = [hairstyle_img, user_img] + result_imgs
            
            # Convert to RGB if necessary
            for i, img in enumerate(all_imgs):
                if img.mode != 'RGB':
                    all_imgs[i] = img.convert('RGB')
            
            # Define target height (use the minimum height among all images, but at least 512px)
            target_height = max(512, min(img.height for img in all_imgs))
            
            # Resize all images to the same height while maintaining aspect ratio
            def resize_to_height(img, target_height):
                aspect_ratio = img.width / img.height
                target_width = int(target_height * aspect_ratio)
                return img.resize((target_width, target_height), Image.Resampling.LANCZOS)
            
            resized_imgs = [resize_to_height(img, target_height) for img in all_imgs]
            
            # Calculate total width
            total_width = sum(img.width for img in resized_imgs)
            
            # Create new image for the combined result
            combined_img = Image.new('RGB', (total_width, target_height), (255, 255, 255))
            
            # Paste images side by side
            x_offset = 0
            for img in resized_imgs:
                combined_img.paste(img, (x_offset, 0))
                x_offset += img.width
            
            # Save the combined image
            combined_img.save(output_path, 'PNG', quality=95)
            print(f"Combined image saved: {output_path}")
            return True
            
        except Exception as e:
            print(f"Error creating combined image: {e}")
            return False
    
    def resize_image_for_word(self, image_path, max_width=2.5):
        """Resize image to fit in Word document"""
        try:
            with Image.open(image_path) as img:
                width, height = img.size
                aspect_ratio = height / width
                
                if width > max_width * 96:  # 96 DPI default
                    new_width = max_width
                    new_height = new_width * aspect_ratio
                    return new_width, new_height
                else:
                    return width / 96, height / 96
        except:
            return max_width, max_width
    
    def process_single_combination_with_timeout(self, task_info):
        """Process a single user-hairstyle combination with timeout control"""
        start_time = time.time()
        thread_name = threading.current_thread().name
        user_file = task_info[2]
        hairstyle_file = task_info[3]

        try:
            print(f"[{thread_name}] å¼€å§‹å¤„ç†ä»»åŠ¡ (è¶…æ—¶é™åˆ¶: {self.task_timeout}ç§’): {user_file} + {hairstyle_file}")
            result = self.process_single_combination(task_info)
            end_time = time.time()
            elapsed = end_time - start_time

            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if elapsed > self.task_timeout:
                self.timeout_count += 1
                print(f"[{thread_name}] âš ï¸ ä»»åŠ¡è¶…æ—¶ (è€—æ—¶: {elapsed:.2f}ç§’): {user_file} + {hairstyle_file}")
                return None

            print(f"[{thread_name}] ä»»åŠ¡å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’: {user_file} + {hairstyle_file}")
            return result

        except Exception as e:
            end_time = time.time()
            elapsed = end_time - start_time
            print(f"[{thread_name}] âŒ ä»»åŠ¡å¼‚å¸¸ (è€—æ—¶: {elapsed:.2f}ç§’): {user_file} + {hairstyle_file}")
            print(f"[{thread_name}] å¼‚å¸¸è¯¦æƒ…: {e}")
            return None

    def process_single_combination(self, task_info):
        """Process a single user-hairstyle combination with Gemini preprocessing"""
        user_full_path, hairstyle_full_path, user_file, hairstyle_file, gender_name, results_dir = task_info

        print(f"[{threading.current_thread().name}] Processing: {user_file} + {hairstyle_file}")

        try:
            # Step 1: Geminié¢„å¤„ç†å›¾åƒ
            print(f"[{threading.current_thread().name}] Step 1: Gemini preprocessing...")
            processed_user_path, processed_hairstyle_path = self.preprocess_images_concurrently(
                user_full_path, hairstyle_full_path
            )

            # Step 2: Upload processed images
            print(f"[{threading.current_thread().name}] Step 2: Uploading processed images...")
            user_filename = self.upload_image(processed_user_path)
            if not user_filename:
                print(f"[{threading.current_thread().name}] Failed to upload user image, trying original...")
                user_filename = self.upload_image(user_full_path)
                if not user_filename:
                    return

            hairstyle_filename = self.upload_image(processed_hairstyle_path)
            if not hairstyle_filename:
                print(f"[{threading.current_thread().name}] Failed to upload hairstyle image, trying original...")
                hairstyle_filename = self.upload_image(hairstyle_full_path)
                if not hairstyle_filename:
                    return
            
            # Run task
            print(f"[{threading.current_thread().name}] Running hairstyle transfer task...")
            task_id = self.run_hairstyle_task(hairstyle_filename, user_filename)
            if not task_id:
                return
            
            # Wait for completion
            print(f"[{threading.current_thread().name}] Task {task_id} started, waiting for completion...")
            max_wait = 1000  # 5 minutes max
            wait_time = 0
            
            while wait_time < max_wait:
                status = self.check_task_status(task_id)
                if status == "SUCCESS":
                    break
                elif status in ["FAILED", "CANCELLED"]:
                    print(f"[{threading.current_thread().name}] Task failed with status: {status}")
                    return
                
                time.sleep(10)
                wait_time += 10
                if wait_time % 10 == 0:  # Print every 10 seconds
                    print(f"[{threading.current_thread().name}] Still processing... ({wait_time}s)")
            
            if status != "SUCCESS":
                print(f"[{threading.current_thread().name}] Task did not complete successfully: {status}")
                return
            
            # Get results
            print(f"[{threading.current_thread().name}] Getting results...")
            results = self.get_task_results(task_id)
            if not results:
                return
            
            # Download result images and create combined images
            result_paths = []
            result_filenames = []
            
            # Download all result images first
            for i, result in enumerate(results):
                result_url = result.get("fileUrl")
                if result_url:
                    result_filename = f"{gender_name}_{user_file}_{hairstyle_file}_result_{i}.png"
                    result_path = os.path.join(results_dir, result_filename)
                    
                    if self.download_image(result_url, result_path):
                        result_paths.append(result_path)
                        result_filenames.append(result_filename)
            
            # Create one combined image with all results (original hairstyle + original user + results)
            if result_paths:
                combined_filename = f"{gender_name}_{user_file}_{hairstyle_file}_combined_all.png"
                combined_path = os.path.join(results_dir, combined_filename)

                # Use original images for the combined image to show the transformation
                if self.create_combined_image(hairstyle_full_path, user_full_path, result_paths, combined_path):
                    print(f"[{threading.current_thread().name}] Created combined image: {combined_filename}")

                # Store result info (thread-safe) - include both original and processed paths
                with self.results_lock:
                    self.results.append({
                        'gender': gender_name,
                        'user_image': user_full_path,  # ä¿ç•™åŸå§‹è·¯å¾„ç”¨äºè®°å½•
                        'hairstyle_image': hairstyle_full_path,  # ä¿ç•™åŸå§‹è·¯å¾„ç”¨äºè®°å½•
                        'processed_user_image': processed_user_path,  # æ–°å¢é¢„å¤„ç†è·¯å¾„
                        'processed_hairstyle_image': processed_hairstyle_path,  # æ–°å¢é¢„å¤„ç†è·¯å¾„
                        'result_images': result_paths,
                        'combined_image': combined_path if os.path.exists(combined_path) else None,
                        'user_filename': user_file,
                        'hairstyle_filename': hairstyle_file,
                        'result_filenames': result_filenames,
                        'combined_filename': combined_filename
                    })
            
            print(f"[{threading.current_thread().name}] Completed: {user_file} + {hairstyle_file}")
            
        except Exception as e:
            print(f"[{threading.current_thread().name}] Error processing {user_file} + {hairstyle_file}: {e}")
    
    def process_gender_folder(self, gender_path, gender_name):
        """Process all combinations for a gender (man/woman) with concurrent processing"""
        hairstyle_path = os.path.join(gender_path, "hairstyle")
        user_path = os.path.join(gender_path, "user")
        
        if not os.path.exists(hairstyle_path) or not os.path.exists(user_path):
            print(f"Missing hairstyle or user folder for {gender_name}")
            return
        
        hairstyle_files = [f for f in os.listdir(hairstyle_path) if f.lower().endswith(('.jpg', '.jpeg', '.png','.JPG', '.JPEG', '.PNG'))]
        user_files = [f for f in os.listdir(user_path) if f.lower().endswith(('.jpg', '.jpeg', '.png','.JPG', '.JPEG', '.PNG'))]
        
        # # For women, randomly select 50 hairstyles
        # if gender_name == "woman" and len(hairstyle_files) > 50:
        #     hairstyle_files = random.sample(hairstyle_files, 50)
        #     print(f"Randomly selected 50 hairstyles for women from {len(os.listdir(hairstyle_path))} total")
        
        print(f"Processing {gender_name}: {len(hairstyle_files)} hairstyles Ã— {len(user_files)} users = {len(hairstyle_files) * len(user_files)} combinations")
        
        results_dir = os.path.join(self.data_dir, f"results_{gender_name}_{datetime.now().strftime('%m%d')}_")
        os.makedirs(results_dir, exist_ok=True)
        
        # Create task list
        tasks = []
        for user_file in user_files:
            for hairstyle_file in hairstyle_files:
                user_full_path = os.path.join(user_path, user_file)
                hairstyle_full_path = os.path.join(hairstyle_path, hairstyle_file)
                task_info = (user_full_path, hairstyle_full_path, user_file, hairstyle_file, gender_name, results_dir)
                tasks.append(task_info)
                # break
        
        # Process tasks concurrently
        print(f"Starting concurrent processing with {self.max_workers} workers (timeout: {self.task_timeout}s per task)...")
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks
            future_to_task = {executor.submit(self.process_single_combination_with_timeout, task): task for task in tasks}

            # Process completed tasks
            completed = 0
            successful = 0
            failed = 0
            timeout_tasks = 0

            for future in concurrent.futures.as_completed(future_to_task, timeout=None):
                completed += 1
                task = future_to_task[future]
                user_file, hairstyle_file = task[2], task[3]

                try:
                    # ä½¿ç”¨ä»»åŠ¡çº§åˆ«çš„è¶…æ—¶
                    result = future.result(timeout=self.task_timeout + 30)  # ç»™é¢å¤–30ç§’çš„ç¼“å†²æ—¶é—´
                    if result is not None:
                        successful += 1
                        print(f"âœ… Progress: {completed}/{len(tasks)} - Success: {successful}, Failed: {failed}, Timeout: {timeout_tasks}")
                    else:
                        failed += 1
                        print(f"âŒ Progress: {completed}/{len(tasks)} - Success: {successful}, Failed: {failed}, Timeout: {timeout_tasks}")

                except concurrent.futures.TimeoutError:
                    timeout_tasks += 1
                    failed += 1
                    print(f"â° Future timeout: {user_file} + {hairstyle_file}")
                    print(f"âš ï¸ Progress: {completed}/{len(tasks)} - Success: {successful}, Failed: {failed}, Timeout: {timeout_tasks}")

                except Exception as exc:
                    failed += 1
                    print(f"ğŸ’¥ Task {user_file} + {hairstyle_file} generated an exception: {exc}")
                    print(f"âŒ Progress: {completed}/{len(tasks)} - Success: {successful}, Failed: {failed}, Timeout: {timeout_tasks}")
            
            print(f"\n=== å¤„ç†å®Œæˆç»Ÿè®¡ ===")
            print(f"æ€»ä»»åŠ¡æ•°: {len(tasks)}")
            print(f"æˆåŠŸå®Œæˆ: {successful}")
            print(f"å¤±è´¥ä»»åŠ¡: {failed}")
            print(f"è¶…æ—¶ä»»åŠ¡: {self.timeout_count}")
            print(f"æˆåŠŸç‡: {(successful/len(tasks)*100):.1f}%")
            print(f"===================")
        
        print(f"Completed processing {gender_name} folder")
    
    def create_word_document(self, output_path="hairstyle_results.docx"):
        """Create Word document with all results"""
        doc = Document()
        doc.add_heading('å‘å‹æ¢è£…ç»“æœ', 0)
        
        doc.add_paragraph(f'ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        doc.add_paragraph(f'æ€»å…±å¤„ç†: {len(self.results)} ä¸ªç»„åˆ')
        
        for i, result in enumerate(self.results):
            doc.add_heading(f'ç»“æœ {i+1}: {result["gender"]} - {result["user_filename"]} + {result["hairstyle_filename"]}', level=1)
            
            # Add combined image if available
            if result.get('combined_image') and os.path.exists(result['combined_image']):
                doc.add_paragraph('æ‹¼æ¥å›¾ç‰‡ (å‘å‹å‚è€ƒ + ç”¨æˆ·ç…§ç‰‡ + ç”Ÿæˆç»“æœ):')
                width, height = self.resize_image_for_word(result['combined_image'], max_width=6.0)  # Wider for combined image
                paragraph = doc.add_paragraph()
                run = paragraph.add_run()
                run.add_picture(result['combined_image'], width=Inches(width), height=Inches(height))
                doc.add_paragraph()  # Add some space
            
            # Create table for individual images
            doc.add_paragraph('å•ç‹¬å›¾ç‰‡:')
            result_images = result.get('result_images', [])
            num_cols = 2 + len(result_images)  # hairstyle + user + result images
            table = doc.add_table(rows=2, cols=num_cols)
            table.style = 'Table Grid'
            
            # Headers
            hdr_cells = table.rows[0].cells
            hdr_cells[0].text = 'å‘å‹å‚è€ƒå›¾'
            hdr_cells[1].text = 'ç”¨æˆ·ç…§ç‰‡'
            for j in range(len(result_images)):
                hdr_cells[2 + j].text = f'ç”Ÿæˆç»“æœ{j+1}'
            
            # Images
            img_cells = table.rows[1].cells
            
            # Add hairstyle image
            if os.path.exists(result['hairstyle_image']):
                width, height = self.resize_image_for_word(result['hairstyle_image'])
                paragraph = img_cells[0].paragraphs[0]
                run = paragraph.runs[0] if paragraph.runs else paragraph.add_run()
                run.add_picture(result['hairstyle_image'], width=Inches(width), height=Inches(height))
            
            # Add user image
            if os.path.exists(result['user_image']):
                width, height = self.resize_image_for_word(result['user_image'])
                paragraph = img_cells[1].paragraphs[0]
                run = paragraph.runs[0] if paragraph.runs else paragraph.add_run()
                run.add_picture(result['user_image'], width=Inches(width), height=Inches(height))
            
            # Add result images
            for j, result_image in enumerate(result_images):
                if os.path.exists(result_image):
                    width, height = self.resize_image_for_word(result_image)
                    paragraph = img_cells[2 + j].paragraphs[0]
                    run = paragraph.runs[0] if paragraph.runs else paragraph.add_run()
                    run.add_picture(result_image, width=Inches(width), height=Inches(height))
            
            doc.add_page_break()
        
        doc.save(output_path)
        print(f"Word document saved: {output_path}")

    def get_cache_info(self):
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        cache_info = {
            'user': {'total_files': 0, 'total_size': 0, 'files': []},
            'hairstyle': {'total_files': 0, 'total_size': 0, 'files': []}
        }

        for image_type in ['user', 'hairstyle']:
            cache_dir = os.path.join(self.data_dir, f"gemini_processed_{image_type}")
            if os.path.exists(cache_dir):
                try:
                    for filename in os.listdir(cache_dir):
                        if filename == 'cache_index.json':
                            continue
                        filepath = os.path.join(cache_dir, filename)
                        if os.path.isfile(filepath):
                            file_stat = os.stat(filepath)
                            cache_info[image_type]['files'].append({
                                'filename': filename,
                                'filepath': filepath,
                                'size': file_stat.st_size,
                                'modified_time': file_stat.st_mtime,
                                'created_time': file_stat.st_ctime
                            })
                            cache_info[image_type]['total_size'] += file_stat.st_size
                            cache_info[image_type]['total_files'] += 1
                except Exception as e:
                    print(f"è·å–{image_type}ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")

        return cache_info

    def clean_old_cache(self, max_age_hours=24, max_total_size_mb=100):
        """æ¸…ç†æ—§çš„ç¼“å­˜æ–‡ä»¶"""
        current_time = time.time()
        max_age_seconds = max_age_hours * 3600
        max_total_size_bytes = max_total_size_mb * 1024 * 1024

        total_cleaned_files = 0
        total_cleaned_size = 0

        for image_type in ['user', 'hairstyle']:
            cache_dir = os.path.join(self.data_dir, f"gemini_processed_{image_type}")
            if not os.path.exists(cache_dir):
                continue

            try:
                cache_index_path = os.path.join(cache_dir, "cache_index.json")
                cache_index = {}

                # è¯»å–ç¼“å­˜ç´¢å¼•
                if os.path.exists(cache_index_path):
                    try:
                        with open(cache_index_path, 'r', encoding='utf-8') as f:
                            cache_index = json.load(f)
                    except:
                        cache_index = {}

                # è·å–æ‰€æœ‰ç¼“å­˜æ–‡ä»¶ä¿¡æ¯
                cache_files = []
                for filename in os.listdir(cache_dir):
                    if filename == 'cache_index.json':
                        continue
                    filepath = os.path.join(cache_dir, filename)
                    if os.path.isfile(filepath):
                        file_stat = os.stat(filepath)
                        cache_files.append({
                            'filename': filename,
                            'filepath': filepath,
                            'size': file_stat.st_size,
                            'modified_time': file_stat.st_mtime
                        })

                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæ—§çš„åœ¨å‰ï¼‰
                cache_files.sort(key=lambda x: x['modified_time'])

                cleaned_files_in_type = 0
                cleaned_size_in_type = 0

                # è®¡ç®—æ€»å¤§å°
                total_size = sum(f['size'] for f in cache_files)

                # æ¸…ç†ç­–ç•¥1: åˆ é™¤è¶…è¿‡æŒ‡å®šæ—¶é—´çš„æ–‡ä»¶
                files_to_remove = []
                for file_info in cache_files:
                    file_age = current_time - file_info['modified_time']
                    if file_age > max_age_seconds:
                        files_to_remove.append(file_info)

                # æ¸…ç†ç­–ç•¥2: å¦‚æœæ€»å¤§å°è¶…è¿‡é™åˆ¶ï¼Œåˆ é™¤æœ€æ—§çš„æ–‡ä»¶
                if total_size > max_total_size_bytes:
                    remaining_files = [f for f in cache_files if f not in files_to_remove]
                    remaining_size = sum(f['size'] for f in remaining_files)

                    for file_info in remaining_files:
                        if remaining_size <= max_total_size_bytes:
                            break
                        files_to_remove.append(file_info)
                        remaining_size -= file_info['size']

                # æ‰§è¡Œåˆ é™¤æ“ä½œ
                for file_info in files_to_remove:
                    try:
                        os.remove(file_info['filepath'])
                        cleaned_files_in_type += 1
                        cleaned_size_in_type += file_info['size']

                        # ä»ç¼“å­˜ç´¢å¼•ä¸­ç§»é™¤å¯¹åº”æ¡ç›®
                        filename_hash = None
                        for hash_key, index_info in cache_index.items():
                            if index_info.get('processed_path') == file_info['filepath']:
                                filename_hash = hash_key
                                break

                        if filename_hash:
                            del cache_index[filename_hash]

                        print(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶: {file_info['filename']} ({file_info['size'] / 1024:.1f}KB)")

                    except Exception as e:
                        print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_info['filepath']}: {e}")

                # æ›´æ–°ç¼“å­˜ç´¢å¼•
                if cleaned_files_in_type > 0:
                    try:
                        with open(cache_index_path, 'w', encoding='utf-8') as f:
                            json.dump(cache_index, f, ensure_ascii=False, indent=2)
                    except Exception as e:
                        print(f"æ›´æ–°{image_type}ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")

                total_cleaned_files += cleaned_files_in_type
                total_cleaned_size += cleaned_size_in_type

                if cleaned_files_in_type > 0:
                    print(f"æ¸…ç†{image_type}ç¼“å­˜: {cleaned_files_in_type}ä¸ªæ–‡ä»¶, {cleaned_size_in_type / 1024:.1f}KB")

            except Exception as e:
                print(f"æ¸…ç†{image_type}ç¼“å­˜ç›®å½•å¤±è´¥: {e}")

        if total_cleaned_files > 0:
            print(f"ç¼“å­˜æ¸…ç†å®Œæˆ: æ€»è®¡åˆ é™¤{total_cleaned_files}ä¸ªæ–‡ä»¶, {total_cleaned_size / 1024:.1f}KB")
        else:
            print("æ— éœ€æ¸…ç†ç¼“å­˜æ–‡ä»¶")

        return {
            'cleaned_files': total_cleaned_files,
            'cleaned_size': total_cleaned_size
        }

    def get_disk_usage(self):
        """è·å–ç£ç›˜ä½¿ç”¨æƒ…å†µ"""
        try:
            import shutil
            total, used, free = shutil.disk_usage(self.data_dir)
            return {
                'total': total,
                'used': used,
                'free': free,
                'usage_percent': (used / total) * 100
            }
        except Exception as e:
            print(f"è·å–ç£ç›˜ä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
            return None

    def delete_cache_file(self, file_path, image_type):
        """åˆ é™¤æŒ‡å®šçš„ç¼“å­˜æ–‡ä»¶"""
        try:
            # éªŒè¯æ–‡ä»¶è·¯å¾„æ˜¯å¦åœ¨ç¼“å­˜ç›®å½•å†…ï¼ˆå®‰å…¨æ£€æŸ¥ï¼‰
            cache_dir = os.path.join(self.data_dir, f"gemini_processed_{image_type}")
            normalized_file_path = os.path.normpath(file_path)
            normalized_cache_dir = os.path.normpath(cache_dir)

            if not normalized_file_path.startswith(normalized_cache_dir):
                print(f"å®‰å…¨æ£€æŸ¥å¤±è´¥: æ–‡ä»¶è·¯å¾„ä¸åœ¨ç¼“å­˜ç›®å½•å†… {file_path}")
                return False

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False

            # è·å–æ–‡ä»¶å¤§å°ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
            file_size = os.path.getsize(file_path)

            # åˆ é™¤æ–‡ä»¶
            os.remove(file_path)

            # ä»ç¼“å­˜ç´¢å¼•ä¸­ç§»é™¤å¯¹åº”æ¡ç›®
            cache_index_path = os.path.join(cache_dir, "cache_index.json")
            if os.path.exists(cache_index_path):
                try:
                    with open(cache_index_path, 'r', encoding='utf-8') as f:
                        cache_index = json.load(f)

                    # æŸ¥æ‰¾å¹¶åˆ é™¤å¯¹åº”çš„ç´¢å¼•æ¡ç›®
                    hash_to_remove = None
                    for hash_key, index_info in cache_index.items():
                        if index_info.get('processed_path') == file_path:
                            hash_to_remove = hash_key
                            break

                    if hash_to_remove:
                        del cache_index[hash_to_remove]

                        # æ›´æ–°ç´¢å¼•æ–‡ä»¶
                        with open(cache_index_path, 'w', encoding='utf-8') as f:
                            json.dump(cache_index, f, ensure_ascii=False, indent=2)

                except Exception as e:
                    print(f"æ›´æ–°ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")

            print(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶æˆåŠŸ: {os.path.basename(file_path)} ({file_size / 1024:.1f}KB)")
            return True

        except Exception as e:
            print(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return False

    def get_cache_files_detailed(self):
        """è·å–è¯¦ç»†çš„ç¼“å­˜æ–‡ä»¶åˆ—è¡¨"""
        cache_files = {
            'user': [],
            'hairstyle': []
        }

        for image_type in ['user', 'hairstyle']:
            cache_dir = os.path.join(self.data_dir, f"gemini_processed_{image_type}")
            if os.path.exists(cache_dir):
                try:
                    # è¯»å–ç¼“å­˜ç´¢å¼•
                    cache_index_path = os.path.join(cache_dir, "cache_index.json")
                    cache_index = {}
                    if os.path.exists(cache_index_path):
                        try:
                            with open(cache_index_path, 'r', encoding='utf-8') as f:
                                cache_index = json.load(f)
                        except:
                            cache_index = {}

                    # è·å–æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
                    for filename in os.listdir(cache_dir):
                        if filename == 'cache_index.json':
                            continue

                        filepath = os.path.join(cache_dir, filename)
                        if os.path.isfile(filepath):
                            try:
                                file_stat = os.stat(filepath)

                                # æŸ¥æ‰¾å¯¹åº”çš„åŸå§‹æ–‡ä»¶ä¿¡æ¯
                                original_filename = None
                                original_path = None
                                for hash_key, index_info in cache_index.items():
                                    if index_info.get('processed_path') == filepath:
                                        original_filename = index_info.get('original_filename', 'Unknown')
                                        original_path = index_info.get('original_path', '')
                                        break

                                cache_files[image_type].append({
                                    'filename': filename,
                                    'filepath': filepath,
                                    'original_filename': original_filename or filename,
                                    'original_path': original_path or '',
                                    'size': file_stat.st_size,
                                    'size_mb': file_stat.st_size / (1024 * 1024),
                                    'modified_time': file_stat.st_mtime,
                                    'created_time': file_stat.st_ctime,
                                    'modified_time_str': datetime.fromtimestamp(file_stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                                    'created_time_str': datetime.fromtimestamp(file_stat.st_ctime).strftime('%Y-%m-%d %H:%M:%S')
                                })
                            except Exception as e:
                                print(f"è·å–æ–‡ä»¶ {filename} ä¿¡æ¯å¤±è´¥: {e}")

                    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæ–°çš„åœ¨å‰ï¼‰
                    cache_files[image_type].sort(key=lambda x: x['modified_time'], reverse=True)

                except Exception as e:
                    print(f"è·å–{image_type}ç¼“å­˜æ–‡ä»¶è¯¦æƒ…å¤±è´¥: {e}")

        return cache_files

    def get_average_task_time(self):
        """è®¡ç®—å¹¶æ˜¾ç¤ºrun_hairstyle_taskå’ŒGeminié¢„å¤„ç†çš„ç»Ÿè®¡ä¿¡æ¯"""

        # RunningHubä»»åŠ¡ç»Ÿè®¡
        if not self.task_times:
            print("æ²¡æœ‰RunningHubä»»åŠ¡è¿è¡Œè®°å½•")
            runninghub_avg = 0.0
        else:
            total_time = sum(self.task_times)
            runninghub_avg = total_time / len(self.task_times)
            min_time = min(self.task_times)
            max_time = max(self.task_times)

            print(f"\n=== RunningHubä»»åŠ¡ç»Ÿè®¡ ===")
            print(f"æ€»ä»»åŠ¡æ•°: {len(self.task_times)}")
            print(f"æ€»è¿è¡Œæ—¶é—´: {total_time:.2f}ç§’")
            print(f"å¹³å‡è¿è¡Œæ—¶é—´: {runninghub_avg:.2f}ç§’")
            print(f"æœ€çŸ­è¿è¡Œæ—¶é—´: {min_time:.2f}ç§’")
            print(f"æœ€é•¿è¿è¡Œæ—¶é—´: {max_time:.2f}ç§’")
            print(f"========================\n")

        # Geminié¢„å¤„ç†ç»Ÿè®¡
        if not self.gemini_times:
            print("æ²¡æœ‰Geminié¢„å¤„ç†è®°å½•")
            gemini_avg = 0.0
        else:
            total_gemini_time = sum(self.gemini_times)
            gemini_avg = total_gemini_time / len(self.gemini_times)
            min_gemini_time = min(self.gemini_times)
            max_gemini_time = max(self.gemini_times)

            print(f"=== Geminié¢„å¤„ç†ç»Ÿè®¡ ===")
            print(f"æ€»é¢„å¤„ç†è¯·æ±‚æ•°: {len(self.gemini_times)}")
            print(f"æˆåŠŸå¤„ç†æ•°: {self.gemini_success_count}")
            print(f"å¤±è´¥å¤„ç†æ•°: {self.gemini_fail_count}")
            print(f"æˆåŠŸç‡: {(self.gemini_success_count / (self.gemini_success_count + self.gemini_fail_count) * 100):.1f}%" if (self.gemini_success_count + self.gemini_fail_count) > 0 else "N/A")
            print(f"æ€»é¢„å¤„ç†æ—¶é—´: {total_gemini_time:.2f}ç§’")
            print(f"å¹³å‡é¢„å¤„ç†æ—¶é—´: {gemini_avg:.2f}ç§’")
            print(f"æœ€çŸ­é¢„å¤„ç†æ—¶é—´: {min_gemini_time:.2f}ç§’")
            print(f"æœ€é•¿é¢„å¤„ç†æ—¶é—´: {max_gemini_time:.2f}ç§’")
            print(f"========================\n")

        # ç»¼åˆç»Ÿè®¡
        total_processed_combinations = len(self.results)
        if total_processed_combinations > 0 or self.timeout_count > 0:
            print(f"=== ç»¼åˆå¤„ç†ç»Ÿè®¡ ===")
            print(f"å¤„ç†çš„å›¾åƒç»„åˆæ•°: {total_processed_combinations}")
            print(f"è¶…æ—¶ä»»åŠ¡æ•°: {self.timeout_count}")
            if self.timeout_count > 0:
                total_attempts = total_processed_combinations + self.timeout_count
                print(f"ä»»åŠ¡æˆåŠŸç‡: {(total_processed_combinations/total_attempts*100):.1f}%")
                print(f"ä»»åŠ¡è¶…æ—¶ç‡: {(self.timeout_count/total_attempts*100):.1f}%")
            print(f"å¹³å‡RunningHubä»»åŠ¡æ—¶é—´: {runninghub_avg:.2f}ç§’")
            print(f"å¹³å‡Geminié¢„å¤„ç†æ—¶é—´: {gemini_avg:.2f}ç§’")
            print(f"ä»»åŠ¡è¶…æ—¶é™åˆ¶: {self.task_timeout}ç§’")
            print(f"===================\n")

        return runninghub_avg

def main():
    hair_base_path = "/Users/alex_wu/work/hair"
    
    # Set random seed for reproducible results
    random.seed(42)
    
    # åˆ›å»ºå¤„ç†å™¨ï¼Œè®¾ç½®è¶…æ—¶æ—¶é—´ä¸º30åˆ†é’Ÿï¼ˆ1800ç§’ï¼‰
    processor = HairstyleProcessor(max_workers=2, task_timeout=600)
    
    # Process women's hairstyles (with random selection of 50)
    woman_path = os.path.join(hair_base_path, "woman")
    if os.path.exists(woman_path):
        print("Starting women's hairstyle processing...")
        processor.process_gender_folder(woman_path, "woman")
    
    # Process men's hairstyles
    man_path = os.path.join(hair_base_path, "man")
    if os.path.exists(man_path):
        print("Starting men's hairstyle processing...")
        processor.process_gender_folder(man_path, "man")

    # Create Word document with all results
    # if processor.results:
    #     processor.create_word_document("hairstyle_results.docx")
    #     print(f"Processing complete! Generated {len(processor.results)} results.")
    # else:
    #     print("No results generated.")
    
    # æ˜¾ç¤ºä»»åŠ¡è¿è¡Œæ—¶é—´ç»Ÿè®¡
    processor.get_average_task_time()

if __name__ == "__main__":
    main()